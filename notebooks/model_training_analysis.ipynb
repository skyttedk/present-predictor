{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gavefabrikken Demand Prediction - Model Training & Analysis\n",
    "\n",
    "This notebook demonstrates the complete pipeline for training the XGBoost demand prediction model using historical gift selection data.\n",
    "\n",
    "## Overview\n",
    "- **Data Loading**: Load historical gift selection data with robust encoding handling\n",
    "- **Data Preprocessing**: Aggregate selection events and prepare features\n",
    "- **Model Training**: Train XGBoost model for demand prediction\n",
    "- **Evaluation**: Analyze model performance and feature importance\n",
    "- **Business Insights**: Extract actionable insights from the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to the path\n",
    "project_root = os.path.abspath('..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Import our custom modules\n",
    "from src.ml.model import DemandPredictor\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading with Robust Encoding Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load historical data with robust encoding handling\n",
    "historical_data_path = \"../src/data/historical/present.selection.historic.csv\"\n",
    "\n",
    "print(\"ðŸ“‚ Loading historical data with robust encoding handling...\")\n",
    "\n",
    "# Try different encodings\n",
    "encodings_to_try = ['utf-8', 'latin-1', 'iso-8859-1', 'cp1252', 'utf-16']\n",
    "raw_data = None\n",
    "\n",
    "for encoding in encodings_to_try:\n",
    "    try:\n",
    "        raw_data = pd.read_csv(historical_data_path, encoding=encoding)\n",
    "        print(f\"âœ… Successfully loaded with {encoding} encoding\")\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"âŒ Failed with {encoding} encoding\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with {encoding}: {e}\")\n",
    "        continue\n",
    "\n",
    "if raw_data is None:\n",
    "    raise ValueError(f\"Could not load CSV with any encoding: {encodings_to_try}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Raw Data Shape: {raw_data.shape}\")\n",
    "print(f\"ðŸ“Š Total selection events: {len(raw_data)}\")\n",
    "print(f\"ðŸ“Š Features: {raw_data.shape[1]} columns\")\n",
    "\n",
    "print(\"\\nðŸ” First 5 rows:\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "print(\"ðŸ§¹ Cleaning data...\")\n",
    "\n",
    "# Remove quotes and clean string columns\n",
    "string_columns = raw_data.select_dtypes(include=['object']).columns\n",
    "for col in string_columns:\n",
    "    raw_data[col] = raw_data[col].astype(str).str.strip('\"').str.strip()\n",
    "\n",
    "# Handle missing values\n",
    "raw_data = raw_data.fillna(\"NONE\")\n",
    "\n",
    "# Standardize categorical values\n",
    "categorical_columns = ['employee_gender', 'product_target_gender', 'product_utility_type', 'product_durability', 'product_type']\n",
    "for col in categorical_columns:\n",
    "    if col in raw_data.columns:\n",
    "        raw_data[col] = raw_data[col].str.lower()\n",
    "\n",
    "print(\"âœ… Data cleaned successfully!\")\n",
    "print(f\"\\nðŸ“Š Data Summary:\")\n",
    "print(f\"Total selection events: {len(raw_data)}\")\n",
    "print(f\"Unique employees by gender: {raw_data['employee_gender'].value_counts().to_dict()}\")\n",
    "print(f\"Unique product categories: {raw_data['product_main_category'].nunique()}\")\n",
    "print(f\"Unique brands: {raw_data['product_brand'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate selection events by counting occurrences\n",
    "print(\"ðŸ”„ Aggregating selection events...\")\n",
    "\n",
    "# Define grouping columns (all categorical features)\n",
    "grouping_columns = [\n",
    "    'employee_shop', 'employee_branch', 'employee_gender',\n",
    "    'product_main_category', 'product_sub_category', 'product_brand',\n",
    "    'product_color', 'product_durability', 'product_target_gender',\n",
    "    'product_utility_type', 'product_type'\n",
    "]\n",
    "\n",
    "# Aggregate by counting selection events\n",
    "aggregated_data = raw_data.groupby(grouping_columns).size().reset_index(name='selection_count')\n",
    "\n",
    "print(f\"\\nðŸ“Š Aggregation Results:\")\n",
    "print(f\"Original events: {len(raw_data)} â†’ Unique combinations: {len(aggregated_data)}\")\n",
    "print(f\"This means {len(raw_data) - len(aggregated_data)} events were duplicates\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Selection Count Distribution:\")\n",
    "selection_dist = aggregated_data['selection_count'].value_counts().sort_index()\n",
    "print(selection_dist)\n",
    "\n",
    "print(\"\\nðŸ” First 10 aggregated combinations:\")\n",
    "aggregated_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training features using label encoding\n",
    "print(\"âš™ï¸ Creating training features with label encoding...\")\n",
    "\n",
    "# Separate features and target\n",
    "X = aggregated_data[grouping_columns].copy()\n",
    "y = aggregated_data['selection_count']\n",
    "\n",
    "# Label encode categorical features\n",
    "label_encoders = {}\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        X[column] = le.fit_transform(X[column].astype(str))\n",
    "        label_encoders[column] = le\n",
    "\n",
    "print(f\"\\nðŸ“Š Training Data Shape:\")\n",
    "print(f\"Features (X): {X.shape}\")\n",
    "print(f\"Target (y): {y.shape}\")\n",
    "print(f\"Sample-to-feature ratio: {len(X) / len(X.columns):.1f}:1\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Target Variable Statistics:\")\n",
    "print(f\"Mean selections: {y.mean():.2f}\")\n",
    "print(f\"Max selections: {y.max()}\")\n",
    "print(f\"Min selections: {y.min()}\")\n",
    "\n",
    "print(\"\\nðŸ” Encoded Features (first 5 rows):\")\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show label encoder mappings\n",
    "print(\"ðŸ·ï¸ Label Encoder Mappings:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for column, encoder in label_encoders.items():\n",
    "    mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))\n",
    "    print(f\"\\n{column}:\")\n",
    "    for original, encoded in list(mapping.items())[:5]:  # Show first 5\n",
    "        print(f\"  '{original}' â†’ {encoded}\")\n",
    "    if len(mapping) > 5:\n",
    "        print(f\"  ... and {len(mapping) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the XGBoost model\n",
    "print(\"ðŸš€ Training XGBoost model...\")\n",
    "\n",
    "model = DemandPredictor()\n",
    "training_stats = model.train(X, y, validation_split=0.2)\n",
    "\n",
    "print(\"\\nâœ… Training completed!\")\n",
    "\n",
    "# Display training results\n",
    "if training_stats.get('small_dataset_warning', False):\n",
    "    print(\"\\nâš ï¸ Small dataset detected - used full dataset for training\")\n",
    "    print(\"ðŸ“Š Training Metrics:\")\n",
    "    for metric, value in training_stats['train_metrics'].items():\n",
    "        print(f\"  {metric.upper()}: {value:.4f}\")\nelse:\n",
    "    print(\"\\nðŸ“Š Training Metrics:\")\n",
    "    for metric, value in training_stats['train_metrics'].items():\n",
    "        print(f\"  {metric.upper()}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š Validation Metrics:\")\n",
    "    for metric, value in training_stats['validation_metrics'].items():\n",
    "        print(f\"  {metric.upper()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance\n",
    "print(\"ðŸŽ¯ Feature Importance Analysis\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "feature_importance = model.get_feature_importance()\n",
    "\n",
    "print(\"\\nðŸ† Feature Importance Rankings:\")\n",
    "for i, (feature, importance) in enumerate(list(feature_importance.items()), 1):\n",
    "    print(f\"  {i:2d}. {feature}: {importance:.6f} ({importance*100:.2f}%)\")\n",
    "\n",
    "# Check if we have meaningful feature importance\n",
    "max_importance = max(feature_importance.values()) if feature_importance else 0\n",
    "print(f\"\\nðŸ“Š Maximum feature importance: {max_importance:.6f}\")\n",
    "\n",
    "if max_importance > 0.01:\n",
    "    print(\"\\nâœ… Great! We have meaningful feature importance values.\")\n",
    "    print(\"This indicates the model learned patterns from the data.\")\nelse:\n",
    "    print(\"\\nâš ï¸ Feature importance values are very low.\")\n",
    "    print(\"This might indicate insufficient training data or data quality issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Get top features for plotting\n",
    "top_n = min(10, len(feature_importance))\n",
    "top_features = dict(list(feature_importance.items())[:top_n])\n",
    "features = list(top_features.keys())\n",
    "importance_scores = list(top_features.values())\n",
    "\n",
    "# Create horizontal bar plot\n",
    "y_pos = np.arange(len(features))\n",
    "plt.barh(y_pos, importance_scores, color='steelblue', alpha=0.7)\n",
    "plt.yticks(y_pos, features)\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.title(f'Top {top_n} Feature Importance (XGBoost Model)')\n",
    "plt.gca().invert_yaxis()  # Highest importance at top\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(importance_scores):\n",
    "    plt.text(v + max(importance_scores)*0.01, i, f'{v:.3f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ðŸ“Š Plotted top {top_n} features by importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions and evaluate\n",
    "print(\"ðŸ”® Making predictions and evaluating model...\")\n",
    "\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Actual': y.values,\n",
    "    'Predicted': predictions,\n",
    "    'Difference': y.values - predictions,\n",
    "    'Abs_Error': np.abs(y.values - predictions)\n",
    "})\n",
    "\n",
    "print(\"\\nðŸ“Š Prediction Statistics:\")\n",
    "print(f\"Mean Absolute Error: {comparison_df['Abs_Error'].mean():.3f}\")\n",
    "print(f\"Max Error: {comparison_df['Abs_Error'].max():.3f}\")\n",
    "print(f\"Mean Actual: {comparison_df['Actual'].mean():.3f}\")\n",
    "print(f\"Mean Predicted: {comparison_df['Predicted'].mean():.3f}\")\n",
    "\n",
    "print(\"\\nðŸ” Sample Predictions (first 10):\")\n",
    "print(comparison_df.head(10).round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actual\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Scatter plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y, predictions, alpha=0.7, color='darkblue')\n",
    "plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2)  # Perfect prediction line\n",
    "plt.xlabel('Actual Selection Count')\n",
    "plt.ylabel('Predicted Selection Count')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "plt.subplot(1, 2, 2)\n",
    "residuals = y - predictions\n",
    "plt.scatter(predictions, residuals, alpha=0.7, color='darkgreen')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted Selection Count')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.title('Residuals Plot')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Business Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract business insights\n",
    "print(\"ðŸ¢ BUSINESS INSIGHTS FROM MODEL\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Top features with business interpretation\n",
    "print(\"\\nðŸŽ¯ Key Factors Driving Gift Selection:\")\n",
    "top_5_features = list(feature_importance.items())[:5]\n",
    "\n",
    "business_interpretations = {\n",
    "    'product_main_category': 'Gift type/category is the primary driver',\n",
    "    'product_durability': 'Durability (consumable vs durable) affects choice',\n",
    "    'employee_branch': 'Location/branch influences preferences',\n",
    "    'employee_shop': 'Shop/company culture affects selections',\n",
    "    'employee_gender': 'Gender influences gift preferences',\n",
    "    'product_target_gender': 'Target demographic matters for selection',\n",
    "    'product_utility_type': 'Utility type (practical/aesthetic) drives choice',\n",
    "    'product_brand': 'Brand preference affects selection',\n",
    "    'product_sub_category': 'Specific subcategory influences choice'\n",
    "}\n",
    "\n",
    "for i, (feature, importance) in enumerate(top_5_features, 1):\n",
    "    interpretation = business_interpretations.get(feature, 'Significant factor in gift selection')\n",
    "    print(f\"  {i}. {feature} ({importance*100:.1f}%): {interpretation}\")\n",
    "\n",
    "# Category analysis\n",
    "print(\"\\nðŸ“Š Popular Product Categories:\")\n",
    "category_popularity = raw_data['product_main_category'].value_counts().head(5)\n",
    "for category, count in category_popularity.items():\n",
    "    print(f\"  â€¢ {category}: {count} selections\")\n",
    "\n",
    "# Gender preferences\n",
    "print(\"\\nðŸ‘¥ Employee Demographics:\")\n",
    "gender_dist = raw_data['employee_gender'].value_counts()\n",
    "for gender, count in gender_dist.items():\n",
    "    print(f\"  â€¢ {gender.title()}: {count} selections ({count/len(raw_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_save_path = \"../models/demand_predictor_production.pkl\"\n",
    "print(f\"ðŸ’¾ Saving model to {model_save_path}...\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "Path(model_save_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model and metadata\n",
    "model.save_model(model_save_path)\n",
    "print(\"âœ… Model saved successfully!\")\n",
    "\n",
    "# Save label encoders for future use\n",
    "import pickle\n",
    "encoders_path = \"../models/label_encoders.pkl\"\n",
    "with open(encoders_path, 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "print(f\"âœ… Label encoders saved to {encoders_path}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Model Summary:\")\n",
    "print(f\"â€¢ Training samples: {len(X)}\")\n",
    "print(f\"â€¢ Features: {len(X.columns)}\")\n",
    "print(f\"â€¢ Model type: XGBoost Regressor\")\n",
    "print(f\"â€¢ Ready for production use: âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"ðŸ“‹ TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nâœ… Successfully trained XGBoost demand prediction model!\")\n",
    "print(f\"\\nðŸ“Š Dataset Statistics:\")\n",
    "print(f\"â€¢ Raw selection events: {len(raw_data)}\")\n",
    "print(f\"â€¢ Unique combinations: {len(aggregated_data)}\")\n",
    "print(f\"â€¢ Features used: {len(X.columns)}\")\n",
    "print(f\"â€¢ Sample-to-feature ratio: {len(X) / len(X.columns):.1f}:1\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Model Performance:\")\n",
    "if not training_stats.get('small_dataset_warning', False):\n",
    "    val_r2 = training_stats['validation_metrics']['r2']\n",
    "    print(f\"â€¢ Validation RÂ²: {val_r2:.4f}\")\n",
    "else:\n",
    "    train_r2 = training_stats['train_metrics']['r2']\n",
    "    print(f\"â€¢ Training RÂ²: {train_r2:.4f}\")\n",
    "\n",
    "max_importance = max(feature_importance.values())\n",
    "print(f\"â€¢ Max feature importance: {max_importance:.3f}\")\n",
    "print(f\"â€¢ Feature importance quality: {'Good' if max_importance > 0.1 else 'Moderate' if max_importance > 0.01 else 'Low'}\")\n",
    "\n",
    "print(f\"\\nðŸš€ Ready for Production:\")\n",
    "print(f\"â€¢ Model saved: {model_save_path}\")\n",
    "print(f\"â€¢ Encoders saved: {encoders_path}\")\n",
    "print(f\"â€¢ API integration: Ready\")\n",
    "print(f\"â€¢ Prediction pipeline: Complete\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(f\"â€¢ Integrate model with FastAPI endpoints\")\n",
    "print(f\"â€¢ Connect to three-step processing pipeline\")\n",
    "print(f\"â€¢ Deploy for real-time demand predictions\")\n",
    "print(f\"â€¢ Monitor model performance with new data\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Training pipeline completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}