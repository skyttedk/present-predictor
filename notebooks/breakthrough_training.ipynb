{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakthrough Model Training - With Shop Assortment Features\n",
    "\n",
    "This notebook reproduces the breakthrough performance (CV R² ≈ 0.2947) and incorporates new shop assortment features to achieve further improvement (target CV R² ≈ 0.3112).\n",
    "\n",
    "**Key Steps:**\n",
    "1. Load and preprocess historical data.\n",
    "2. Aggregate data to unique product-employee combinations.\n",
    "3. **NEW:** Perform EDA and engineer shop-level assortment proxy features (non-leaky).\n",
    "4. Prepare final feature set (original + new shop features) and target variables (including log transformation).\n",
    "5. Apply stratified cross-validation.\n",
    "6. Train XGBoost model with optimal parameters.\n",
    "7. Evaluate performance and feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Add src to path for imports if any utilities were there\n",
    "# sys.path.append('../') \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[DATA] Loading historical selection data...\")\n",
    "data_path = \"../src/data/historical/present.selection.historic.csv\"\n",
    "try:\n",
    "    raw_data = pd.read_csv(data_path, encoding='utf-8', dtype='str')\n",
    "    print(f\"Loaded {len(raw_data)} selection events from {data_path}\")\n",
    "    print(f\"Columns: {list(raw_data.columns)}\")\n",
    "    # display(raw_data.head()) # Keep display for interactive runs\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: {data_path} not found.\")\n",
    "    raw_data = pd.DataFrame() \n",
    "\n",
    "if not raw_data.empty:\n",
    "    print(\"\\n[CLEAN] Cleaning data...\")\n",
    "    cleaned_data = raw_data.copy()\n",
    "    for col in cleaned_data.columns:\n",
    "        cleaned_data[col] = cleaned_data[col].astype(str).str.strip('\"').str.strip()\n",
    "    cleaned_data = cleaned_data.fillna(\"NONE\")\n",
    "    categorical_cols_to_lower = ['employee_gender', 'product_target_gender', \n",
    "                       'product_utility_type', 'product_durability', 'product_type']\n",
    "    for col in categorical_cols_to_lower:\n",
    "        if col in cleaned_data.columns:\n",
    "            cleaned_data[col] = cleaned_data[col].str.lower()\n",
    "    print(f\"Data cleaning complete: {len(cleaned_data)} records\")\n",
    "    # display(cleaned_data.head())\n",
    "else:\n",
    "    print(\"Raw data is empty, skipping cleaning.\")\n",
    "    cleaned_data = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Aggregation (Unique Product-Employee Combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_data = pd.DataFrame() # Initialize\n",
    "grouping_cols = []\n",
    "if not cleaned_data.empty:\n",
    "    grouping_cols = [\n",
    "        'employee_shop', 'employee_branch', 'employee_gender',\n",
    "        'product_main_category', 'product_sub_category', 'product_brand',\n",
    "        'product_color', 'product_durability', 'product_target_gender',\n",
    "        'product_utility_type', 'product_type'\n",
    "    ]\n",
    "    print(f\"\\n[AGGREGATE] Grouping by {len(grouping_cols)} features: {grouping_cols}\")\n",
    "    agg_data = cleaned_data.groupby(grouping_cols).size().reset_index(name='selection_count')\n",
    "    if not agg_data.empty:\n",
    "        compression_ratio = len(cleaned_data) / len(agg_data) if len(agg_data) > 0 else 0\n",
    "        print(f\"Aggregation complete:\")\n",
    "        print(f\"  {len(cleaned_data)} events → {len(agg_data)} unique combinations\")\n",
    "        print(f\"  Compression ratio: {compression_ratio:.1f}x\")\n",
    "        # display(agg_data.head())\n",
    "    else:\n",
    "        print(\"agg_data is empty after grouping.\")\n",
    "else:\n",
    "    print(\"Cleaned data is empty, skipping aggregation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Shop Assortment Feature Engineering (EDA and New Feature Creation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 EDA for Shop-Level Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_summary = pd.DataFrame()\n",
    "shop_top_main_cats = pd.DataFrame()\n",
    "shop_top_brands = pd.DataFrame()\n",
    "agg_data_with_shop_features = pd.DataFrame() # Initialize\n",
    "\n",
    "if not agg_data.empty:\n",
    "    print(\"\\nPerforming EDA for Shop-Level Patterns...\")\n",
    "    shop_summary = agg_data.groupby('employee_shop').agg(\n",
    "        total_shop_selections=('selection_count', 'sum'),\n",
    "        unique_product_combinations_in_shop=('product_main_category', 'count'),\n",
    "        distinct_main_categories_in_shop=('product_main_category', 'nunique'),\n",
    "        distinct_sub_categories_in_shop=('product_sub_category', 'nunique'),\n",
    "        distinct_brands_in_shop=('product_brand', 'nunique'),\n",
    "        distinct_utility_types_in_shop=('product_utility_type', 'nunique')\n",
    "    ).reset_index()\n",
    "    print(\"\\nShop Summary Statistics (EDA):\")\n",
    "    # display(shop_summary.head())\n",
    "    # display(shop_summary.describe())\n",
    "\n",
    "    shop_main_cat_counts = agg_data.groupby(['employee_shop', 'product_main_category'])['selection_count'].sum().reset_index()\n",
    "    idx = shop_main_cat_counts.groupby(['employee_shop'])['selection_count'].transform(max) == shop_main_cat_counts['selection_count']\n",
    "    shop_top_main_cats = shop_main_cat_counts[idx].drop_duplicates(subset=['employee_shop'], keep='first')\n",
    "    shop_top_main_cats = shop_top_main_cats[['employee_shop', 'product_main_category']].rename(\n",
    "        columns={'product_main_category': 'shop_most_frequent_main_category_selected'}\n",
    "    )\n",
    "    # print(\"\\nShop Most Frequent Main Category (Selected):\")\n",
    "    # display(shop_top_main_cats.head())\n",
    "\n",
    "    shop_brand_counts = agg_data.groupby(['employee_shop', 'product_brand'])['selection_count'].sum().reset_index()\n",
    "    idx_brand = shop_brand_counts.groupby(['employee_shop'])['selection_count'].transform(max) == shop_brand_counts['selection_count']\n",
    "    shop_top_brands = shop_brand_counts[idx_brand].drop_duplicates(subset=['employee_shop'], keep='first')\n",
    "    shop_top_brands = shop_top_brands[['employee_shop', 'product_brand']].rename(\n",
    "        columns={'product_brand': 'shop_most_frequent_brand_selected'}\n",
    "    )\n",
    "    # print(\"\\nShop Most Frequent Brand (Selected):\")\n",
    "    # display(shop_top_brands.head())\n",
    "    print(\"EDA for shop patterns complete.\")\n",
    "else:\n",
    "    print(\"Skipping EDA as agg_data is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Engineer Shop-Level Aggregate Features (Non-Leaky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_features_df = pd.DataFrame() # Initialize\n",
    "\n",
    "if not shop_summary.empty:\n",
    "    print(\"\\nEngineering Shop-Level Aggregate Features...\")\n",
    "    shop_features_df = shop_summary.copy()\n",
    "    if not shop_top_main_cats.empty:\n",
    "        shop_features_df = pd.merge(shop_features_df, shop_top_main_cats, on='employee_shop', how='left')\n",
    "    if not shop_top_brands.empty:\n",
    "        shop_features_df = pd.merge(shop_features_df, shop_top_brands, on='employee_shop', how='left')\n",
    "    \n",
    "    # Rename columns for clarity (some were already good from shop_summary)\n",
    "    shop_features_df = shop_features_df.rename(columns={\n",
    "        'distinct_main_categories_in_shop': 'shop_main_category_diversity_selected',\n",
    "        'distinct_brands_in_shop': 'shop_brand_diversity_selected',\n",
    "        'distinct_utility_types_in_shop': 'shop_utility_type_diversity_selected',\n",
    "        'distinct_sub_categories_in_shop': 'shop_sub_category_diversity_selected' # Added this for completeness\n",
    "    })\n",
    "    # Drop features that might be too close to target if used directly or redundant after other calculations\n",
    "    # 'total_shop_selections' and 'shop_avg_selections_per_product_combination' were found to be problematic.\n",
    "    # We will rely on diversity and frequency features instead.\n",
    "    cols_to_potentially_drop_from_shop_features = ['total_shop_selections']\n",
    "    # Example: also drop 'shop_avg_selections_per_product_combination' if it was created in shop_summary and needs removal\n",
    "    # if 'shop_avg_selections_per_product_combination' in shop_features_df.columns: \n",
    "    #    cols_to_potentially_drop_from_shop_features.append('shop_avg_selections_per_product_combination')\n",
    "    shop_features_df = shop_features_df.drop(columns=[col for col in cols_to_potentially_drop_from_shop_features if col in shop_features_df.columns], errors='ignore')\n",
    "\n",
    "    print(\"Shop-level features created and refined:\")\n",
    "    # display(shop_features_df.head())\n",
    "    \n",
    "    if not agg_data.empty:\n",
    "        agg_data_with_shop_features = pd.merge(agg_data, shop_features_df, on='employee_shop', how='left')\n",
    "        print(\"\\nAgg_data with shop-level features merged.\")\n",
    "        # display(agg_data_with_shop_features.head())\n",
    "    else:\n",
    "        print(\"agg_data is empty, cannot merge shop features.\")\n",
    "else:\n",
    "    print(\"Skipping Shop-Level Aggregate Features as shop_summary is empty.\")\n",
    "    agg_data_with_shop_features = agg_data.copy() # Proceed with original agg_data if no shop features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Engineer Product-Relative-to-Shop Features (Non-Leaky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features_df = pd.DataFrame() # Initialize\n",
    "\n",
    "if not agg_data_with_shop_features.empty:\n",
    "    print(\"\\nEngineering Product-Relative-to-Shop Features (Non-Leaky)...\")\n",
    "    df = agg_data_with_shop_features.copy()\n",
    "\n",
    "    if 'shop_most_frequent_main_category_selected' in df.columns:\n",
    "        df['is_shop_most_frequent_main_category'] = (\n",
    "            df['product_main_category'] == df['shop_most_frequent_main_category_selected']\n",
    "        ).astype(int)\n",
    "    else:\n",
    "        df['is_shop_most_frequent_main_category'] = 0 # Default if shop-level feature wasn't created\n",
    "\n",
    "    if 'shop_most_frequent_brand_selected' in df.columns:\n",
    "        df['is_shop_most_frequent_brand'] = (\n",
    "            df['product_brand'] == df['shop_most_frequent_brand_selected']\n",
    "        ).astype(int)\n",
    "    else:\n",
    "        df['is_shop_most_frequent_brand'] = 0\n",
    "\n",
    "    # Leaky features related to selection_count_* are intentionally omitted here.\n",
    "    # Examples of previously leaky features (now removed from creation):\n",
    "    # df['selection_count_rank_in_shop'] = df.groupby('employee_shop')['selection_count'].rank(method='dense', ascending=False)\n",
    "    # df['selection_count_share_in_shop'] = df['selection_count'] / df['total_shop_selections']\n",
    "    # df['selection_count_vs_shop_avg'] = df['selection_count'] - df['shop_avg_selections_per_product_combination']\n",
    "\n",
    "    print(\"Non-leaky product-relative-to-shop features created.\")\n",
    "    # display(df.head())\n",
    "    \n",
    "    final_features_df = df.copy()\n",
    "    print(f\"\\nShape of final_features_df: {final_features_df.shape}\")\n",
    "    # print(\"Columns in final_features_df:\")\n",
    "    # for col in final_features_df.columns: print(f\"  - {col}\")\n",
    "else:\n",
    "    print(\"Skipping Product-Relative-to-Shop Features as input df is empty.\")\n",
    "    final_features_df = agg_data.copy() # Fallback to agg_data if previous steps failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Feature Preparation for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = None\n",
    "y = None\n",
    "y_log = None\n",
    "y_strata = None\n",
    "label_encoders = {}\n",
    "\n",
    "if not final_features_df.empty and 'selection_count' in final_features_df.columns:\n",
    "    print(\"[FEATURES PREP] Preparing final X, y, y_log, y_strata...\")\n",
    "    y = final_features_df['selection_count']\n",
    "    y_log = np.log1p(y)\n",
    "    X = final_features_df.drop(columns=['selection_count']).copy()\n",
    "    \n",
    "    print(f\"Original number of features in X before encoding: {X.shape[1]}\")\n",
    "    \n",
    "    new_shop_categorical_features = [\n",
    "        'shop_most_frequent_main_category_selected',\n",
    "        'shop_most_frequent_brand_selected'\n",
    "    ]\n",
    "    all_categorical_cols_for_encoding = []\n",
    "    if 'grouping_cols' in globals() and isinstance(grouping_cols, list):\n",
    "        all_categorical_cols_for_encoding = list(set(grouping_cols + new_shop_categorical_features))\n",
    "    else:\n",
    "        print(\"Warning: grouping_cols not found or not a list. Will only use new_shop_categorical_features for determining categorical columns.\")\n",
    "        all_categorical_cols_for_encoding = new_shop_categorical_features\n",
    "\n",
    "    print(\"\\n[ENCODE] Label encoding categorical features...\")\n",
    "    for col in X.columns:\n",
    "        if col in all_categorical_cols_for_encoding and X[col].dtype == 'object':\n",
    "            X[col] = X[col].astype(str)\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col])\n",
    "            label_encoders[col] = le\n",
    "        elif pd.api.types.is_numeric_dtype(X[col]):\n",
    "            if X[col].isnull().any():\n",
    "                X[col] = X[col].fillna(X[col].median())\n",
    "        elif X[col].dtype == 'object':\n",
    "            X[col] = X[col].astype(str)\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col])\n",
    "            label_encoders[col] = le\n",
    "\n",
    "    X = X.fillna(0)\n",
    "    for col in X.columns:\n",
    "        if not pd.api.types.is_numeric_dtype(X[col]):\n",
    "            print(f\"Warning: Column {col} is not numeric after processing (dtype: {X[col].dtype}). Attempting conversion or dropping.\")\n",
    "            try:\n",
    "                X[col] = pd.to_numeric(X[col])\n",
    "            except ValueError:\n",
    "                print(f\"Could not convert {col} to numeric, dropping column.\")\n",
    "                X = X.drop(columns=[col])\n",
    "                \n",
    "    y_strata = pd.cut(y, bins=[0, 1, 2, 5, 10, np.inf], labels=[0, 1, 2, 3, 4], include_lowest=True)\n",
    "    \n",
    "    print(f\"\\nFinal X features shape: {X.shape}\")\n",
    "    print(f\"Target shapes: Original y={y.shape}, Log y_log={y_log.shape}\")\n",
    "    if not y_strata.empty:\n",
    "      print(f\"Stratification distribution:\\n{y_strata.value_counts().sort_index().to_dict()}\")\n",
    "    if X is not None and X.shape[1] > 0:\n",
    "        print(f\"Sample-to-feature ratio: {len(X) / X.shape[1]:.1f}:1\")\n",
    "        print(\"\\nDEBUG: Columns in X before training:\", list(X.columns))\n",
    "        print(f\"DEBUG: Shape of X before training: {X.shape}\")\n",
    "    elif X is not None:\n",
    "        print(\"X DataFrame has 0 columns after processing.\")\n",
    "else:\n",
    "    print(\"Skipping final feature preparation as final_features_df is empty or 'selection_count' is missing.\")\n",
    "    X = pd.DataFrame()\n",
    "    y = pd.Series(dtype='float64')\n",
    "    y_log = pd.Series(dtype='float64')\n",
    "    y_strata = pd.Series(dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_cv_mean_log = 0.0\n",
    "r2_cv_std_log = 0.0\n",
    "r2_val_log = 0.0\n",
    "overfitting_log = 0.0\n",
    "mae_original_val = 0.0\n",
    "rmse_original_val = 0.0\n",
    "r2_original_val_scale = 0.0\n",
    "trained_model = None\n",
    "\n",
    "if not X.empty and not y_log.empty and not y_strata.empty:\n",
    "    print(\"\\n[MODEL TRAINING] Training XGBoost with new shop features...\")\n",
    "    optimal_xgb_params = {\n",
    "        'n_estimators': 1000, 'max_depth': 6, 'learning_rate': 0.03,\n",
    "        'subsample': 0.9, 'colsample_bytree': 0.9, 'reg_alpha': 0.3, 'reg_lambda': 0.3,\n",
    "        'gamma': 0.1, 'min_child_weight': 8, 'random_state': 42, 'n_jobs': -1\n",
    "    }\n",
    "    model = XGBRegressor(**optimal_xgb_params)\n",
    "\n",
    "    # Stratified Cross-Validation\n",
    "    cv_stratified = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Ensure no NaNs in strata for CV splitting\n",
    "    valid_cv_indices = y_strata.dropna().index\n",
    "    X_for_cv = X.loc[valid_cv_indices]\n",
    "    y_log_for_cv = y_log.loc[valid_cv_indices]\n",
    "    y_strata_for_cv = y_strata.loc[valid_cv_indices]\n",
    "\n",
    "    if len(X_for_cv) > 0 and len(y_strata_for_cv) == len(X_for_cv):\n",
    "        cv_scores = cross_val_score(model, X_for_cv, y_log_for_cv, \n",
    "                                    cv=cv_stratified.split(X_for_cv, y_strata_for_cv), \n",
    "                                    scoring='r2')\n",
    "        r2_cv_mean_log = cv_scores.mean()\n",
    "        r2_cv_std_log = cv_scores.std()\n",
    "        print(f\"Stratified CV R² (on y_log): {r2_cv_mean_log:.4f} ± {r2_cv_std_log:.4f}\")\n",
    "    else:\n",
    "        print(\"Not enough valid data for CV after handling NaNs in strata or strata mismatch.\")\n",
    "\n",
    "    # Train/Validation Split for other metrics\n",
    "    # Ensure stratify uses a clean y_strata that matches X and y_log length\n",
    "    # We'll use the same valid_cv_indices for consistency if there were NaNs\n",
    "    X_train, X_val, y_log_train, y_log_val, y_original_train, y_original_val = train_test_split(\n",
    "        X_for_cv, y_log_for_cv, y.loc[valid_cv_indices], \n",
    "        test_size=0.2, random_state=42, stratify=y_strata_for_cv\n",
    "    )\n",
    "    \n",
    "    trained_model = XGBRegressor(**optimal_xgb_params)\n",
    "    trained_model.fit(X_train, y_log_train)\n",
    "    \n",
    "    y_log_pred_val = trained_model.predict(X_val)\n",
    "    r2_val_log = r2_score(y_log_val, y_log_pred_val)\n",
    "    print(f\"Validation R² (on y_log): {r2_val_log:.4f}\")\n",
    "    \n",
    "    overfitting_log = r2_val_log - r2_cv_mean_log\n",
    "    print(f\"Overfitting (Validation R² - CV R²): {overfitting_log:+.4f}\")\n",
    "\n",
    "    # Metrics on Original Scale\n",
    "    y_pred_val_original_scale = np.expm1(y_log_pred_val)\n",
    "    y_pred_val_original_scale = np.maximum(0, y_pred_val_original_scale)\n",
    "    \n",
    "    mae_original_val = mean_absolute_error(y_original_val, y_pred_val_original_scale)\n",
    "    mse_original_val = mean_squared_error(y_original_val, y_pred_val_original_scale)\n",
    "    rmse_original_val = np.sqrt(mse_original_val)\n",
    "    r2_original_val_scale = r2_score(y_original_val, y_pred_val_original_scale)\n",
    "\n",
    "    print(f\"\\nMetrics on Original Scale (Validation Set):\")\n",
    "    print(f\"  MAE: {mae_original_val:.4f}\")\n",
    "    print(f\"  RMSE: {rmse_original_val:.4f}\")\n",
    "    print(f\"  R² (original scale): {r2_original_val_scale:.4f}\")\n",
    "    \n",
    "    baseline_r2 = 0.2947 # From original breakthrough\n",
    "    improvement = r2_cv_mean_log - baseline_r2\n",
    "    print(f\"\\nImprovement in CV R² (log target) over baseline ({baseline_r2}): {improvement:+.4f}\")\n",
    "else:\n",
    "    print(\"Skipping Model Training as X, y_log or y_strata is empty/invalid.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if trained_model is not None and not X.empty:\n",
    "    print(\"\\n[ANALYSIS] Feature Importance Analysis\")\n",
    "    feature_importances = trained_model.feature_importances_\n",
    "    feature_names = X.columns # Use columns from the final X used for training\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance Ranking (New Model with Shop Features):\")\n",
    "    for i, row in importance_df.iterrows():\n",
    "        print(f\"  {i+1:2d}. {row['feature']:<45} {row['importance']:.4f}\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, max(8, len(importance_df) * 0.3)))\n",
    "    sns.barplot(x='importance', y='feature', data=importance_df, palette='viridis')\n",
    "    plt.title('XGBoost Feature Importance - Model with Shop Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping feature importance as model was not trained or X is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Persistence (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_threshold = 0.30 # Save if CV R2 (log) is better than this\n",
    "if trained_model is not None and r2_cv_mean_log > save_model_threshold:\n",
    "    print(f\"\\n[SAVE] New model performance ({r2_cv_mean_log:.4f}) exceeds threshold ({save_model_threshold:.4f}). Saving model...\")\n",
    "    \n",
    "    model_dir = '../models/final_shop_features_model/' # New directory for this version\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    final_model_path = os.path.join(model_dir, 'final_xgb_shop_features_model.pkl')\n",
    "    joblib.dump(trained_model, final_model_path)\n",
    "    print(f\"[OK] Final model saved to: {final_model_path}\")\n",
    "    \n",
    "    final_encoders_path = os.path.join(model_dir, 'final_label_encoders.pkl')\n",
    "    with open(final_encoders_path, 'wb') as f:\n",
    "        pickle.dump(label_encoders, f) \n",
    "    print(f\"[OK] Final label encoders saved to: {final_encoders_path}\")\n",
    "    \n",
    "    final_metadata = {\n",
    "        'model_type': 'XGBoost Regressor',\n",
    "        'description': 'Model trained with refined shop assortment features (non-leaky)',\n",
    "        'source_data_aggregation': 'present.selection.historic.csv aggregated by 11 features',\n",
    "        'target_transformation': 'log1p(selection_count)',\n",
    "        'cv_methodology': 'StratifiedKFold (5 splits) by selection_count bins',\n",
    "        'performance_log_target': {\n",
    "            'stratified_cv_r2_mean': r2_cv_mean_log,\n",
    "            'stratified_cv_r2_std': r2_cv_std_log,\n",
    "            'validation_r2': r2_val_log,\n",
    "            'overfitting': overfitting_log\n",
    "        },\n",
    "        'performance_original_scale_validation': {\n",
    "            'mae': mae_original_val,\n",
    "            'rmse': rmse_original_val,\n",
    "            'r2': r2_original_val_scale\n",
    "        },\n",
    "        'features': list(X.columns) if not X.empty else [],\n",
    "        'training_data_size': len(X_train) if 'X_train' in locals() and not X_train.empty else (len(X_for_cv) if 'X_for_cv' in locals() else 0),\n",
    "        'feature_importance': importance_df.set_index('feature')['importance'].to_dict() if 'importance_df' in locals() and not importance_df.empty else {}\n",
    "    }\n",
    "    final_metadata_path = os.path.join(model_dir, 'final_model_metadata.pkl')\n",
    "    with open(final_metadata_path, 'wb') as f:\n",
    "        pickle.dump(final_metadata, f)\n",
    "    print(f\"[OK] Final model metadata saved to: {final_metadata_path}\")\n",
    "else:\n",
    "    if trained_model is not None:\n",
    "        print(f\"\\n[NO SAVE] New model performance ({r2_cv_mean_log:.4f}) does not exceed threshold ({save_model_threshold:.4f}). Model not saved.\")\n",
    "    else:\n",
    "        print(\"\\n[NO SAVE] Model not trained or performance metrics unavailable. Model not saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary of Final Model with Shop Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL MODEL RESULTS (with Shop Assortment Features - Non-Leaky)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "if trained_model is not None:\n",
    "    print(f\"Stratified CV R² (log target): {r2_cv_mean_log:.4f} ± {r2_cv_std_log:.4f}\")\n",
    "    print(f\"Validation R² (log target): {r2_val_log:.4f}\")\n",
    "    print(f\"Overfitting (log target): {overfitting_log:+.4f}\")\n",
    "    print(\"\\nMetrics on Original Scale (Validation Set):\")\n",
    "    print(f\"  MAE: {mae_original_val:.4f}\")\n",
    "    print(f\"  RMSE: {rmse_original_val:.4f}\")\n",
    "    print(f\"  R² (original scale): {r2_original_val_scale:.4f}\")\n",
    "    \n",
    "    baseline_r2 = 0.2947\n",
    "    improvement = r2_cv_mean_log - baseline_r2\n",
    "    print(f\"\\nImprovement in CV R² (log target) over baseline ({baseline_r2}): {improvement:+.4f}\")\n",
    "    \n",
    "    if improvement > 0.01:\n",
    "        print(\"\\n[CONCLUSION] The new shop assortment features provided a notable improvement.\")\n",
    "    elif improvement > 0:\n",
    "        print(\"\\n[CONCLUSION] The new shop assortment features provided a slight improvement.\")\n",
    "    else:\n",
    "        print(\"\\n[CONCLUSION] The new shop assortment features did not significantly improve or worsened performance compared to baseline.\")\n",
    "else:\n",
    "    print(\"Model training was not completed successfully. Cannot display final results.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12" 
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}