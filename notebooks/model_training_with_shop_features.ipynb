{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training with Shop Assortment Features\n",
    "\n",
    "This notebook implements Phase 2 of the plan outlined in `SHOP_ASSORTMENT_FEATURE_ENGINEERING_PLAN.md`.\n",
    "\n",
    "**Objective:** Retrain the XGBoost model using the augmented dataset that includes new shop-level and product-relative-to-shop features, and evaluate its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Augmented Data\n",
    "\n",
    "Load the `agg_data_with_shop_features.csv` file created by the `shop_assortment_feature_engineering.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../src/data/processed/agg_data_with_shop_features.csv\"\n",
    "print(f\"[DATA] Loading augmented data from: {data_path}...\")\n",
    "try:\n",
    "    augmented_data = pd.read_csv(data_path)\n",
    "    print(f\"Loaded {len(augmented_data)} records.\")\n",
    "    print(f\"Columns: {list(augmented_data.columns)}\")\n",
    "    display(augmented_data.head())\n",
    "    print(f\"Shape: {augmented_data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: {data_path} not found. Please ensure the feature engineering notebook was run successfully.\")\n",
    "    augmented_data = pd.DataFrame() # Ensure augmented_data exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Preparation & Encoding\n",
    "\n",
    "Prepare features (X) and target (y, y_log). This includes identifying all categorical columns (original + new) and applying Label Encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not augmented_data.empty:\n",
    "    print(\"[FEATURES] Preparing features and targets...\")\n",
    "    \n",
    "    # Define target variable\n",
    "    y = augmented_data['selection_count']\n",
    "    y_log = np.log1p(y) # Log-transformed target\n",
    "    \n",
    "    # Define features (X) - all columns except 'selection_count'\n",
    "    X = augmented_data.drop(columns=['selection_count']).copy()\n",
    "    \n",
    "    # Identify all categorical features for encoding\n",
    "    # Original 11 features were all treated as categorical (after ensuring string type)\n",
    "    original_grouping_cols = [\n",
    "        'employee_shop', 'employee_branch', 'employee_gender',\n",
    "        'product_main_category', 'product_sub_category', 'product_brand',\n",
    "        'product_color', 'product_durability', 'product_target_gender',\n",
    "        'product_utility_type', 'product_type'\n",
    "    ]\n",
    "    \n",
    "    # New categorical features from shop_features (example, verify actual columns from previous notebook output)\n",
    "    # These were 'shop_most_frequent_main_category_selected', 'shop_most_frequent_brand_selected'\n",
    "    new_categorical_features = []\n",
    "    if 'shop_most_frequent_main_category_selected' in X.columns:\n",
    "        new_categorical_features.append('shop_most_frequent_main_category_selected')\n",
    "    if 'shop_most_frequent_brand_selected' in X.columns:\n",
    "        new_categorical_features.append('shop_most_frequent_brand_selected')\n",
    "        \n",
    "    all_categorical_cols = list(set(original_grouping_cols + new_categorical_features))\n",
    "    \n",
    "    # Ensure all columns to be encoded are actually in X and are of string type before encoding\n",
    "    # Numerical features (like diversity scores, ranks, shares) should not be in all_categorical_cols\n",
    "    \n",
    "    print(\"\\n[ENCODE] Label encoding categorical features...\")\n",
    "    label_encoders = {}\n",
    "    for col in X.columns:\n",
    "        # Check if column is intended to be categorical based on our list, or if it's object/string type\n",
    "        # This logic might need refinement based on the exact nature of all new features\n",
    "        if col in all_categorical_cols or X[col].dtype == 'object':\n",
    "            print(f\"  Encoding: {col}\")\n",
    "            X[col] = X[col].astype(str) # Ensure string type for encoder\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col])\n",
    "            label_encoders[col] = le\n",
    "        elif pd.api.types.is_numeric_dtype(X[col]):\n",
    "             # For numeric types, fill NaNs if any (e.g. from merges or calculations)\n",
    "            if X[col].isnull().any():\n",
    "                print(f\"  Filling NaNs in numeric column: {col}\")\n",
    "                X[col] = X[col].fillna(X[col].median()) # Or use 0 or mean\n",
    "        else:\n",
    "            print(f\"  Skipping encoding for non-categorical or already numeric: {col} (dtype: {X[col].dtype})\")\n",
    "            \n",
    "    # Handle any remaining NaNs in the feature matrix (e.g., for numeric columns if not caught above)\n",
    "    if X.isnull().values.any():\n",
    "        print(\"\\n[NAN_FILL] Filling remaining NaNs in X with 0 (median or mean might be better for some features)... \")\n",
    "        X = X.fillna(0) # A simple strategy; might need refinement for specific features\n",
    "        \n",
    "    # Create stratification for CV (based on original selection_count)\n",
    "    y_strata = pd.cut(y, bins=[0, 1, 2, 5, 10, np.inf], labels=[0, 1, 2, 3, 4], include_lowest=True)\n",
    "    \n",
    "    print(f\"\\nFeatures shape: {X.shape}\")\n",
    "    print(f\"Target shapes: Original={y.shape}, Log={y_log.shape}\")\n",
    "    print(f\"Stratification distribution:\\n{y_strata.value_counts().sort_index().to_dict()}\")\n",
    "    display(X.head())\n",
    "    print(f\"\\nSample-to-feature ratio: {len(X) / X.shape[1]:.1f}:1\")\n",
    "else:\n",
    "    print(\"Skipping Feature Preparation as augmented_data is empty.\")\n",
    "    X = pd.DataFrame()\n",
    "    y_log = pd.Series()\n",
    "    y_strata = pd.Series()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "Train the XGBoost model with the optimal configuration and evaluate using stratified cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not X.empty:\n",
    "    # Optimal XGBoost configuration (from breakthrough_training.ipynb)\n",
    "    model = XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.03,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=0.3,\n",
    "        reg_lambda=0.3,\n",
    "        gamma=0.1,\n",
    "        min_child_weight=8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n[TRAIN] Training XGBoost model with augmented features on log-transformed target...\")\n",
    "    \n",
    "    # Stratified Cross-Validation\n",
    "    cv_stratified = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Ensure y_strata has no NaNs if y had NaNs not present in X\n",
    "    valid_indices = y_strata.dropna().index\n",
    "    X_cv = X.loc[valid_indices]\n",
    "    y_log_cv = y_log.loc[valid_indices]\n",
    "    y_strata_cv = y_strata.loc[valid_indices]\n",
    "    \n",
    "    if len(X_cv) > 0:\n",
    "        cv_scores = cross_val_score(model, X_cv, y_log_cv, cv=cv_stratified.split(X_cv, y_strata_cv), scoring='r2')\n",
    "        r2_cv_mean = cv_scores.mean()\n",
    "        r2_cv_std = cv_scores.std()\n",
    "        print(f\"Stratified CV R² (on y_log): {r2_cv_mean:.4f} ± {r2_cv_std:.4f}\")\n",
    "    else:\n",
    "        print(\"Not enough data for CV after handling NaNs in strata.\")\n",
    "        r2_cv_mean, r2_cv_std = 0, 0\n",
    "\n",
    "    # Train on full data for validation split and feature importance\n",
    "    X_train, X_val, y_log_train, y_log_val, y_train, y_val = train_test_split(\n",
    "        X, y_log, y, test_size=0.2, random_state=42, stratify=y_strata\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_log_train)\n",
    "    \n",
    "    # Predictions on validation set (log scale)\n",
    "    y_log_pred_val = model.predict(X_val)\n",
    "    r2_val_log = r2_score(y_log_val, y_log_pred_val)\n",
    "    print(f\"Validation R² (on y_log): {r2_val_log:.4f}\")\n",
    "    \n",
    "    # Overfitting check\n",
    "    overfitting = r2_val_log - r2_cv_mean\n",
    "    print(f\"Overfitting (Validation R² - CV R²): {overfitting:+.4f}\")\n",
    "    \n",
    "    # Evaluate on original scale (for MAE, RMSE)\n",
    "    y_pred_val_original_scale = np.expm1(y_log_pred_val)\n",
    "    # Clip negative predictions if any after expm1, though less likely with XGBoost default objective\n",
    "    y_pred_val_original_scale = np.maximum(0, y_pred_val_original_scale) \n",
    "    \n",
    "    mae_original = mean_absolute_error(y_val, y_pred_val_original_scale)\n",
    "    rmse_original = mean_squared_error(y_val, y_pred_val_original_scale, squared=False)\n",
    "    r2_original_val = r2_score(y_val, y_pred_val_original_scale) # R2 on original scale for reference\n",
    "\n",
    "    print(f\"\\nMetrics on Original Scale (Validation Set):\")\n",
    "    print(f\"  MAE: {mae_original:.4f}\")\n",
    "    print(f\"  RMSE: {rmse_original:.4f}\")\n",
    "    print(f\"  R² (original scale): {r2_original_val:.4f}\") \n",
    "    \n",
    "    print(\"\\n--- Compare with Baseline R² (0.2947 on log-transformed target) ---\")\n",
    "    improvement = r2_cv_mean - 0.2947\n",
    "    print(f\"Improvement in CV R² (log target) over baseline: {improvement:+.4f}\")\n",
    "else:\n",
    "    print(\"Skipping Model Training as X is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance Analysis (New Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not X.empty and 'model' in locals():\n",
    "    print(\"\\n[ANALYSIS] Feature Importance for Augmented Model\")\n",
    "    \n",
    "    # Model should have been trained on the full X_train in the previous cell for this analysis\n",
    "    # Or, retrain on the full X for a final importance view (as done in breakthrough notebook)\n",
    "    # For consistency, let's use the model already trained on X_train, y_log_train\n",
    "    # final_model_for_importance = model # if model was trained on full X\n",
    "    # For now, using the model trained on the training split of the CV\n",
    "    \n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_names = X.columns # Use columns from the prepared X\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': feature_importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance Ranking (New Model):\")\n",
    "    for i, row in importance_df.iterrows():\n",
    "        print(f\"  {i+1:2d}. {row['feature']:<45} {row['importance']:.4f}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, max(8, len(importance_df) * 0.3) )) # Adjust height based on num features\n",
    "    sns.barplot(x='importance', y='feature', data=importance_df, palette='viridis')\n",
    "    plt.title('XGBoost Feature Importance - Augmented Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    top_features = importance_df.head(10)\n",
    "    print(f\"\\n[TOP 10] Most Important Features (New Model):\")\n",
    "    for _, row in top_features.iterrows():\n",
    "        print(f\"   {row['feature']:<45} {row['importance']:.4f}\")\n",
    "else:\n",
    "    print(\"Skipping Feature Importance as X is empty or model not trained.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save New Model and Encoders (Optional)\n",
    "\n",
    "If the new model shows significant improvement, save it along with its specific label encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Only save if R2_cv_mean > 0.3 (e.g., better than baseline)\n",
    "save_model_threshold = 0.30 # Adjust as needed\n",
    "\n",
    "if not X.empty and 'model' in locals() and 'r2_cv_mean' in locals() and r2_cv_mean > save_model_threshold:\n",
    "    print(f\"\\n[SAVE] New model performance ({r2_cv_mean:.4f}) exceeds threshold ({save_model_threshold:.4f}). Saving model...\")\n",
    "    \n",
    "    model_dir = '../models/augmented_model/'\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Save the trained model (trained on X_train, y_log_train)\n",
    "    new_model_path = os.path.join(model_dir, 'augmented_xgb_model.pkl')\n",
    "    joblib.dump(model, new_model_path)\n",
    "    print(f\"[OK] Augmented model saved to: {new_model_path}\")\n",
    "    \n",
    "    # Save label encoders used for this augmented dataset\n",
    "    new_encoders_path = os.path.join(model_dir, 'augmented_label_encoders.pkl')\n",
    "    with open(new_encoders_path, 'wb') as f:\n",
    "        pickle.dump(label_encoders, f) # label_encoders from cell 3\n",
    "    print(f\"[OK] Augmented label encoders saved to: {new_encoders_path}\")\n",
    "    \n",
    "    # Save metadata for the new model\n",
    "    new_metadata = {\n",
    "        'model_type': 'XGBoost Regressor',\n",
    "        'description': 'Model trained with shop assortment features',\n",
    "        'source_data': data_path,\n",
    "        'target_transformation': 'log1p',\n",
    "        'cv_methodology': 'Stratified by selection count (5 splits)',\n",
    "        'performance_log_target': {\n",
    "            'stratified_cv_r2_mean': r2_cv_mean,\n",
    "            'stratified_cv_r2_std': r2_cv_std if 'r2_cv_std' in locals() else None,\n",
    "            'validation_r2': r2_val_log if 'r2_val_log' in locals() else None,\n",
    "            'overfitting': overfitting if 'overfitting' in locals() else None\n",
    "        },\n",
    "        'performance_original_scale_validation': {\n",
    "            'mae': mae_original if 'mae_original' in locals() else None,\n",
    "            'rmse': rmse_original if 'rmse_original' in locals() else None,\n",
    "            'r2': r2_original_val if 'r2_original_val' in locals() else None\n",
    "        },\n",
    "        'features': list(X.columns),\n",
    "        'training_data_size': len(X_train) if 'X_train' in locals() else len(X),\n",
    "        'feature_importance': dict(zip(importance_df['feature'], importance_df['importance'])) if 'importance_df' in locals() else None\n",
    "    }\n",
    "    new_metadata_path = os.path.join(model_dir, 'augmented_model_metadata.pkl')\n",
    "    with open(new_metadata_path, 'wb') as f:\n",
    "        pickle.dump(new_metadata, f)\n",
    "    print(f\"[OK] Augmented model metadata saved to: {new_metadata_path}\")\n",
    "else:\n",
    "    if 'r2_cv_mean' in locals():\n",
    "        print(f\"\\n[NO SAVE] New model performance ({r2_cv_mean:.4f}) does not exceed threshold ({save_model_threshold:.4f}). Model not saved.\")\n",
    "    else:\n",
    "        print(\"\\n[NO SAVE] Model not trained or performance metrics unavailable. Model not saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Analyze the R² score and feature importances.\n",
    "2. If performance has improved significantly, this model can become the new production candidate.\n",
    "3. If improvement is modest, consider Phase 3 (Advanced Approaches) from the plan."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}