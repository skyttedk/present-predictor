# Gift Selection Prediction System - Complete Analysis & Roadmap

## Executive Summary

After thorough analysis and architectural review, we've designed an optimal data structure that solves all critical exposure calculation issues. The system now has a clear path forward with completed gender classification API and finalized data architecture.

**Current Status**: 
- âœ… **Gender Classification API**: Completed and ready for data preparation
- âœ… **Optimal Data Architecture**: Finalized three-file structure that solves critical exposure issues
- ðŸš€ **Next Phase**: Implement training pipeline with new data structure

## Optimal Data Architecture (FINALIZED)

### **Data Structure Solution**

We've finalized a **three-file architecture** that completely solves the exposure calculation and data leakage issues:
**Discovery**: One shop can have multiple companies - requires three-file structure for optimal granularity

#### **File 1: present.selection.historic.csv** (Training Events)
```csv
shop_id,company_cvr,employee_gender,gift_id,product_main_category,product_sub_category,product_brand,product_color,product_durability,product_target_gender,product_utility_type,product_type
shop123,12233445,male,gift789,Home & Kitchen,Cookware,Fiskars,NONE,durable,unisex,practical,individual
shop123,12233445,female,gift234,Bags,Toiletry Bag,Markberg,black,durable,female,practical,individual
```

**Purpose**: Each row = ONE historical selection event with company granularity
**Benefits**: Company-level selection patterns, perfect CVR alignment

#### **File 2: shop.catalog.csv** (Available Gifts)
```csv
shop_id,gift_id
shop123,gift789
shop123,gift234
shop123,gift567
shop456,gift789
shop456,gift111
```

**Purpose**: Defines what gifts are available in each shop
**Benefits**: Clean separation of gift availability from company demographics

#### **File 3: company.employees.csv** (Exposure Metadata)
```csv
company_cvr,branch_code,male_count,female_count
12233445,12600,12,34
34446505,12601,10,3
14433445,12600,22,12
```

**Purpose**: Company-specific employee counts for accurate exposure calculation
**Benefits**:
- âœ… **Perfect Granularity**: Company-level modeling (better than shop-level)
- âœ… **CVR Alignment**: Direct match with API request structure
- âœ… **Accurate Exposure**: Gender-specific denominators per company
- âœ… **Zero Data Leakage**: Employee counts are external business metadata

## Business Context Confirmed

- **Selection Rule**: Each employee selects exactly ONE gift from their company's curated selection
- **No Budget Constraints**: Company covers full cost
- **Data Structure**: Each historic CSV row = one selection event
- **Shop Catalogs**: Typically 30-50 gifts per shop
- **Prediction Goal**: How many of each gift will be selected by gender

## Critical Issues - SOLVED BY NEW ARCHITECTURE

### âœ… **1. Exposure Problem - SOLVED**

**Previous Issue**: Wrong exposure calculation using combined counts
**New Solution**:
```python
# Correct gender-specific exposure from company metadata
exposure = company_employees[company_cvr][f"{gender}_count"]
selection_rate = selections / exposure
```

### âœ… **2. Zero-Selection Blindness - SOLVED**

**Previous Issue**: Only learned from selected gifts
**New Solution**: Shop catalog + company employees defines complete universe
- Can identify unselected gifts for each company (zero selections)
- Model learns both popularity and unpopularity patterns per company

### âœ… **3. Data Leakage - ELIMINATED**

**Previous Issue**: Shop features based on selection counts
**New Solution**: Employee counts are external business metadata
- No circular dependencies
- Clean separation of "what was offered" vs "what was selected"

## Completed Components

### âœ… **Gender Classification API** (COMPLETED June 26, 2025)

**Endpoints Available**:
- `POST /classify/gender` - Single name classification
- `POST /classify/gender/batch` - Batch processing (up to 1000 names)

**Features**:
- Enhanced Danish gender classification with fallback support
- API key authentication required
- Performance tracking and confidence scoring
- Ready for external data preparation applications

**Usage for Data Preparation**:
```python
# Prepare employee gender counts for shop catalog
response = requests.post('/classify/gender/batch',
    json={"names": employee_names},
    headers={"X-API-Key": api_key})

male_count = sum(1 for r in response['results'] if r['gender'] == 'male')
female_count = sum(1 for r in response['results'] if r['gender'] == 'female')
```

### âœ… **Optimal API Request Structure** (FINALIZED June 26, 2025)

**New Prediction Request Format (OPTIMAL)**:
```json
{
    "cvr": "28892055",
    "male_count": 12,
    "female_count": 11,
    "presents": [
        {
            "id": "1",
            "description": "Tisvilde Pizzaovn",
            "model_name": "Tisvilde Pizzaovn",
            "model_no": "",
            "vendor": "GaveFabrikken"
        }
    ]
}
```

**Benefits of This Structure**:
- âœ… **Perfect Alignment**: Matches shop catalog structure (`male_count`, `female_count`)
- âœ… **Performance**: No real-time gender classification needed
- âœ… **Accuracy**: Uses known counts vs potentially error-prone name classification
- âœ… **Consistency**: Same data format for training and prediction
- âœ… **Business Logic**: Companies already know their gender distribution
- âœ… **Reliability**: No dependency on classification accuracy

**Impact**: This structure creates perfect consistency between training data and prediction requests, enabling optimal model performance.

## Implementation Roadmap - UPDATED

### Phase 1: Data Pipeline Implementation (Week 1) ðŸš€ **NEXT PRIORITY**

#### 1.1 Training Data Pipeline Update
**Priority**: CRITICAL
**Timeline**: 2-3 days

```python
def load_training_data():
    # Load all three files
    selections = pd.read_csv('present.selection.historic.csv')
    catalog = pd.read_csv('shop.catalog.csv')
    company_employees = pd.read_csv('company.employees.csv')
    
    # Create training records with proper company-level exposure
    training_data = []
    
    for (shop_id, company_cvr, gift_id, gender), group in selections.groupby(['shop_id', 'company_cvr', 'gift_id', 'employee_gender']):
        selection_count = len(group)  # Count of actual selections
        
        # Get exposure from company employees
        company_data = company_employees[company_employees['company_cvr'] == company_cvr]
        if len(company_data) > 0:
            exposure = company_data[f'{gender}_count'].iloc[0]
            
            training_data.append({
                'shop_id': shop_id,
                'company_cvr': company_cvr,
                'gift_id': gift_id,
                'employee_gender': gender,
                'selection_count': selection_count,
                'exposure': exposure,
                'selection_rate': selection_count / exposure,
                **group.iloc[0][product_columns]  # Add product features
            })
    
    # Add zero-selection records for each company-shop-gift combination
    for _, company_row in company_employees.iterrows():
        for _, catalog_row in catalog.iterrows():
            for gender in ['male', 'female']:
                key = (catalog_row['shop_id'], company_row['company_cvr'], catalog_row['gift_id'], gender)
                if key not in selections_set:
                    training_data.append({
                        'shop_id': catalog_row['shop_id'],
                        'company_cvr': company_row['company_cvr'],
                        'gift_id': catalog_row['gift_id'],
                        'employee_gender': gender,
                        'selection_count': 0,
                        'exposure': company_row[f'{gender}_count'],
                        'selection_rate': 0.0,
                        **get_product_features(catalog_row['gift_id'])
                    })
    
    return pd.DataFrame(training_data)
```

#### 1.2 CatBoost Training Updates
**Priority**: CRITICAL
**Timeline**: 2-3 days

```python
def train_catboost_with_proper_exposure(training_df):
    # Use selection_rate as target (not selection_count)
    y = training_df['selection_rate']
    
    # Log exposure for Poisson offset
    log_exposure = np.log(training_df['exposure'] + 1e-8)
    
    # Features (excluding leaky ones)
    feature_columns = [
        'product_main_category', 'product_sub_category', 'product_brand',
        'product_color', 'product_durability', 'product_target_gender',
        'product_utility_type', 'product_type', 'employee_gender',
        'shop_id', 'company_cvr'  # Company-level patterns for better granularity
    ]
    X = training_df[feature_columns]
    
    # Create CatBoost Pool with exposure offset
    train_pool = Pool(
        X, 
        label=y,
        cat_features=['product_main_category', 'product_sub_category', 'product_brand',
                     'product_color', 'product_durability', 'product_target_gender',
                     'product_utility_type', 'product_type', 'employee_gender', 'shop_id', 'company_cvr'],
        baseline=log_exposure  # CRITICAL: Proper Poisson exposure
    )
    
    # Train model (use RMSE for rate prediction, not Poisson)
    model = CatBoostRegressor(
        loss_function='RMSE',  # Predicting rates, not counts
        iterations=1000,
        learning_rate=0.1,
        depth=6,
        random_state=42
    )
    
    model.fit(train_pool)
    return model
```

#### 1.3 Prediction Pipeline Updates
**Priority**: HIGH
**Timeline**: 2 days

```python
class EnhancedGiftPredictor:
    def predict(self, cvr, male_count, female_count, presents):
        """
        Predict gift selections using optimal API request structure with company-level granularity
        
        Args:
            cvr: Company CVR number (direct match with training data)
            male_count: Number of male employees (direct input, matches company.employees.csv)
            female_count: Number of female employees (direct input, matches company.employees.csv)
            presents: List of gift dictionaries with id, description, model_name, etc.
        """
        predictions = []
        shop_id = self.map_cvr_to_shop(cvr)  # Map CVR to internal shop_id
        
        for present in presents:
            # Classify gift attributes from description
            gift_features = self.classify_gift_attributes(present)
            
            for gender in ['male', 'female']:
                # Create feature vector with company granularity
                features = {
                    **gift_features,
                    'employee_gender': gender,
                    'shop_id': shop_id,
                    'company_cvr': cvr  # Company-level modeling
                }
                
                # Get exposure for this gender (direct from API request)
                exposure = male_count if gender == 'male' else female_count
                log_exposure = np.log(exposure + 1e-8)
                
                # Predict selection rate with company context
                predicted_rate = self.model.predict(
                    features,
                    baseline=log_exposure
                )
                
                # Scale by exposure to get expected count
                expected_count = predicted_rate * exposure
                
                predictions.append({
                    'product_id': present['id'],
                    'gender': gender,
                    'predicted_rate': predicted_rate,
                    'expected_count': expected_count,
                    'exposure': exposure
                })
        
        # Aggregate by product_id
        return self.aggregate_by_gift(predictions)
```

### Phase 2: Validation & Testing (Week 2)

#### 2.1 Proper Cross-Validation
```python
# Split by shops to prevent data leakage
from sklearn.model_selection import GroupShuffleSplit

splitter = GroupShuffleSplit(test_size=0.2, n_splits=5, random_state=42)
cv_scores = []

for train_idx, val_idx in splitter.split(training_df, groups=training_df['shop_id']):
    # Train and validate with proper shop separation
    pass
```

#### 2.2 Business Metrics Validation
```python
def validate_business_logic(predictions_df, catalog_df):
    """Validate that predictions make business sense"""
    
    for shop_id in predictions_df['shop_id'].unique():
        shop_preds = predictions_df[predictions_df['shop_id'] == shop_id]
        shop_catalog = catalog_df[catalog_df['shop_id'] == shop_id]
        
        total_predicted = shop_preds['expected_count'].sum()
        total_employees = shop_catalog['male_count'].iloc[0] + shop_catalog['female_count'].iloc[0]
        
        # Should be reasonable (not exactly equal due to choice behavior)
        selection_rate = total_predicted / total_employees
        assert 0.3 <= selection_rate <= 1.2, f"Unrealistic selection rate: {selection_rate}"
```

### Phase 3: API Integration (Week 2-3)

#### 3.1 Update Prediction Endpoint
Update `/predict` endpoint to work with optimal request structure:
- âœ… **Accept Direct Counts**: Use `male_count`, `female_count` from request (no employee names needed)
- **Perfect Alignment**: Request format matches training data structure exactly
- **Enhanced Predictor**: Use proper exposure scaling with direct gender counts
- **Confidence Metrics**: Return predictions with model confidence scores
- **Performance**: No real-time gender classification bottleneck

**New API Endpoint Structure**:
```python
@app.post("/predict")
async def predict_gift_selections(request: OptimalPredictionRequest):
    """
    Predict gift selections using optimal request format
    
    Request format matches training data structure for perfect alignment
    """
    predictions = enhanced_predictor.predict(
        cvr=request.cvr,
        male_count=request.male_count,
        female_count=request.female_count,
        presents=request.presents
    )
    return predictions
```

### Phase 4: Performance Optimization (Week 3)

#### 4.1 Expected Improvements
With the new architecture:
- **Model Performance**: RÂ² improvement from 0.31 to 0.50-0.65
- **Calibration**: Predictions properly scaled by exposure
- **Business Logic**: Sum of predictions reflects actual employee behavior
- **No Data Leakage**: Clean separation of training and prediction data

## Implementation Priority (UPDATED)

1. âœ… **COMPLETED** - Gender Classification API (June 26, 2025)
2. âœ… **COMPLETED** - Optimal API Request Structure with `male_count`/`female_count` (June 26, 2025)
3. ðŸš€ **CRITICAL NEXT** - Data pipeline implementation with three-file structure (Week 1)
4. **CRITICAL** - CatBoost training with proper exposure (Week 1)
5. **HIGH** - Enhanced prediction pipeline with optimal request format (Week 1-2)
6. **HIGH** - Validation with business metrics (Week 2)
7. **MEDIUM** - API endpoint updates with new request structure (Week 2-3)
8. **MEDIUM** - Performance monitoring (Week 3)

## Expected Outcomes

With the optimal data architecture implemented:

1. **Model Performance**:
   - RÂ² improvement from 0.31 to 0.50-0.65
   - Proper calibration (predictions sum to realistic selection rates)
   - Better discrimination between popular/unpopular gifts

2. **Business Impact**:
   - 30-50% reduction in prediction errors
   - Better inventory planning with gender-specific insights
   - Reduced stockouts and overstock situations

3. **Technical Benefits**:
   - Zero data leakage
   - Proper statistical modeling with Poisson exposure
   - Scalable architecture for new shops/gifts
   - Clean separation of concerns

## Next Immediate Steps

1. **Prepare Data Files**: Create the three-file structure (historic + catalog + company employees)
2. **Implement Training Pipeline**: Phase 1.1 and 1.2 with company-level granularity (critical priority)
3. **Update API Schemas**: Implement new request format with CVR and direct `male_count`/`female_count`
4. **Validate Results**: Ensure business logic and performance improvements with company-level modeling
5. **Deploy Enhanced API**: Integrate enhanced predictor with perfect CVR alignment

The architecture is now optimal with perfect training/prediction alignment at company-level granularity and ready for implementation! ðŸš€